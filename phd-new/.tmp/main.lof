\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces High-level architecture of a HPC storage system. Compute nodes are connected to servers through a low-latency network. Each RAID volume protects data against single, or multiple, disk failures. RAID volumes are contained into disk enclosures, each of which is connected to two servers using a storage area network; in this way if one of the servers fails data is still reacheable through the failover node.\relax }}{7}{figure.caption.4}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Contribution to I/O time in a distributed storage system.\relax }}{8}{figure.caption.5}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Simple namespace example and corresponding representation using inodes and data blocks in the file system.\relax }}{10}{figure.caption.6}
\contentsline {figure}{\numberline {2.4}{\ignorespaces High-level architecture of a parallel file system.\relax }}{11}{figure.caption.7}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of namespace distributed across four metadata servers.\relax }}{12}{figure.caption.8}
\contentsline {figure}{\numberline {2.6}{\ignorespaces In this case false sharing of block two in the file can lead to two different outcomes. In the first case process $P_1$ writes before $P_0$, in the second case the order is inverted.\relax }}{14}{figure.caption.9}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Recording density has been improved thanks to technological advancements in different fields including magnetic sensors and recording mediai~\cite {Shiroishi2009}.\relax }}{16}{figure.caption.10}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of two-dimensional domain partitioning in parallel applications. The original domain is divided among four processes using a cyclic-cyclic partitioning strategy. Each process in the application performs its tasks on the data and then writes the results to a shared file concurrently.\relax }}{18}{figure.caption.12}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Common checkpoint patterns in HPC.\relax }}{20}{figure.caption.13}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Classical memory hierarchy in a standard computer. In the figure the dashed boxes represent existing and emerging solid state devices like SSDs and SCMs.\relax }}{22}{figure.caption.14}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Schematic representation of the components of a hard disk drive.\relax }}{23}{figure.caption.15}
\contentsline {figure}{\numberline {2.12}{\ignorespaces High-level architecture of a flash based solid state drive.\relax }}{25}{figure.caption.16}
\contentsline {figure}{\numberline {2.13}{\ignorespaces Effect of readahead on a program that reads sequentially blocks of data from a file in the disk. In the example, every data block $Blk_i$ is read and afterwards processed $P(Blk_i)$ by the application. Readahead can merge the reading of two adjacent blocks amortizing the disk latency.\relax }}{30}{figure.caption.17}
\contentsline {figure}{\numberline {2.14}{\ignorespaces Effect of increased storage bandwidth on prefetching performance.\relax }}{32}{figure.caption.18}
\contentsline {figure}{\numberline {2.15}{\ignorespaces Effect of limited cache space on prefetching performance.\relax }}{34}{figure.caption.19}
\contentsline {figure}{\numberline {2.16}{\ignorespaces High-level architecture of an alternative HPC storage system design that uses I/O nodes to decouple compute from parallel file system access.\relax }}{42}{figure.caption.20}
\contentsline {figure}{\numberline {2.17}{\ignorespaces Simplified two phase I/O scheme. A two-dimensional domain is partitioned among four processes in the parallel application using a column-block strategy. Data is then rearranged to form an intermediate pattern that matches the logical data organization in the file. This pattern is afterwards used by the two aggregators to access the storage system.\relax }}{45}{figure.caption.21}
\contentsline {figure}{\numberline {2.18}{\ignorespaces Pictorial representation of the PLFS translation mechanism that converts original application N-1 patterns into N-N patterns.\relax }}{46}{figure.caption.22}
\contentsline {figure}{\numberline {2.19}{\ignorespaces Simplified representation of the SIONlib translation mechanism. A large number of task-local files is mapped into a smaller number of multifiles.\relax }}{48}{figure.caption.23}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Mercury I/O software stack. \textit {assisted I/O library} and \textit {advice manager} communicate through UNIX domain sockets. The AM binds its socket to the local file system pathname \texttt {/tmp/channel}, while the AIO connects its socket to the same pathname; exactly in the same way they would bind and connect to an IP address if they were located on different nodes in the network. Unix domain sockets are used to pass ancillary data as well as custom messages between the two software entities. Data can reside in a local Linux file system, in Lustre or in GPFS.\relax }}{57}{figure.caption.26}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Detailed architecture for the \textit {Advice Manager} (AM) component. This can be further divided into three blocks: \textit {Request Manager} (RM), \textit {Register Log} (RL), and \textit {Advisor Thread} (AT).\relax }}{59}{figure.caption.27}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Simplified function call graph for the read operation in Lustre. For page operations in the Linux kernel the picture also shows the call graph typically followed by local reads as well as the call graph for the \texttt {POSIX\_FADV\_WILLNEED} advice in the \texttt {posix\_fadvise()} implementation (dashed line).\relax }}{63}{figure.caption.28}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Collective I/O flow diagram for the write path in aggregators (non-aggregators neither receive nor write any data, just send it to aggregators). \texttt {MPI\_File\_write\_all()} invokes \texttt {ADIOI\_GEN\_WriteStridedColl()}. Performance critical functions for the collective I/O branch are highlighted in grey.\relax }}{68}{figure.caption.29}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Extent based locking for GPFS~\ref {figure: gpfs-lock} and Lustre~\ref {figure: lustre-lock}. In both figures there are three process requesting access to different parts of the file at different times.\relax }}{72}{figure.caption.31}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Possible partitioning strategies for GPFS~\ref {figure: gpfs-partition} and Lustre~\ref {figure: lustre-partition}. File domains, and thus aggregators, are marked with different filling patterns.\relax }}{73}{figure.caption.32}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Ideal configuration of processes, I/O servers and data distribution in the system.\relax }}{75}{figure.caption.33}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Effect of I/O server scheduling strategies on collective I/O performance. Three examples are shown, in the first every aggregator reads from a different I/O server; in the second three aggregators read from the same I/O server, which does not perform any scheduling optimization; and in the third three aggregators read from the same I/O server, which this time does perform a scheduling optimization.\relax }}{77}{figure.caption.34}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Six processes are collaborating in collective I/O. Because $P_0$, $P_1$ and $P_2$ do not exchange data with other processes there is no need for them to communicate data shuffling information to $P_3$, $P_4$ and $P_5$ during two phase I/O rounds.\relax }}{79}{figure.caption.35}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Original ROMIO architecture (\ref {figure: romio-architecture}) and proposed ROMIO architecture (\ref {figure: new-romio-architecture}).\relax }}{81}{figure.caption.36}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Cache plugin class diagram. The synchronization thread \texttt {ADIOI\_Sync\_thread\_t} serves synchronization requests of type \texttt {ADIOI\_Sync\_req\_t}.\relax }}{85}{figure.caption.38}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Extended collective I/O flow diagram including cache plugin support.\relax }}{88}{figure.caption.39}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Standard and modified workflows. When cache is disabled compute phase `k+1' starts after file `k' has been closed. When the cache is enabled compute `k+1' can start immediately after data has been written. At the same time, background synchronization of cached data starts. File `k' is closed before the file `k+1' is opened, forcing the implementation to wait for cache synchronization to complete.\relax }}{92}{figure.caption.40}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces I/O read profile of the target application under analysis (\ref {figure: iopat_profile}), extracted from the the GPFS file system in the test cluster, and zoomed window (\ref {figure: iopat_zoom}) showing the actual pattern details.\relax }}{97}{figure.caption.41}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison between different usage stategies of posix\_fadvise for an input file of 55 GB residing in an ext4 file system. The first bar represents the case in which no advice is used, the second bar represents the case in which a POSIX\_FADV\_WILLNEED is issued for the whole file at the beginning of the application and the third bar represents the case in which POSIX\_FADV\_WILLNEED is issued using Mercury.\relax }}{99}{figure.caption.42}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Running time of the ROOT application for the three file systems under study using different input file sized (\ref {figure: ext4_1},~\ref {figure: gpfs_1} and~\ref {figure: lustre_1}).\relax }}{102}{figure.caption.43}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Running time of the ROOT application for the three file system under study using different of application instances accessing a file of 5 GB (\ref {figure: ext4_2},~\ref {figure: gpfs_2} and~\ref {figure: lustre_2}).\relax }}{103}{figure.caption.44}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Reads processed by local ext4, GPFS and Lustre I/O servers for various input file sizes (\ref {figure: ext4_3},~\ref {figure: gpfs_3} and~\ref {figure: lustre_3}).\relax }}{104}{figure.caption.45}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Reads processed by local ext4, GPFS and Lustre I/O servers for multiple instances of ROOT accessing a file of 5 GB (\ref {figure: ext4_4},~\ref {figure: gpfs_4} and~\ref {figure: lustre_4}).\relax }}{105}{figure.caption.46}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Perceived and theoretical write bandwidth for all combinations of aggregators and collective buffer sizes (\ref {figure: collperf-bw}); collective I/O contribution breakdown when cache is disabled (\ref {figure: collperf-elaps-disable}); collective I/O contribution breakdown when cache is enabled (\ref {figure: collperf-elaps-enable}).\relax }}{109}{figure.caption.47}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Perceived I/O bandwidth for all combinations of aggregators and collective buffer sizes (\ref {figure: flash-bw}); collective I/O contribution breakdown when cache is disabled (\ref {figure: flash-elaps-disable}); collective I/O contribution breakdown when cache is enabled (\ref {figure: flash-elaps-enable}).\relax }}{112}{figure.caption.48}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Perceived I/O bandwidth for all combinations of aggregators and collective buffer sizes (\ref {figure: ior-bw}); collective I/O contribution breakdown when cache is disabled (\ref {figure: ior-elaps-disable}); collective I/O contribution breakdown when cache is enabled (\ref {figure: ior-elaps-enable}).\relax }}{114}{figure.caption.49}
\addvspace {10\p@ }
