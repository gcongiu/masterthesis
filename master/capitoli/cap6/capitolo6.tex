\chapter{Valutazione e confronto delle prestazioni ottenute}\label{cap:capitolo6}
In questo capitolo vengono analizzate e messe a confronto le prestazioni ottenute tramite l'impiego, all'interno del simulatore Cellsim, delle due infrastrutture di comunicazione oggetto della tesi. Queste sono state caratterizzate tramite l'ausilio di due benchmark: DMA cycle e DMA memory. Inoltre, per le due infrastrutture, EIB e $\times$pipes, sono state testate diverse configurazioni topologiche in modo da valutare, per ogniuna di esse, i vantaggi ottenibili in termini di latenza dei messaggi e tempo di esecuzione dell'applicazione.   

\section{Configurazioni topologiche adottate per EIB}
Per EIB sono state scelte e testate le seguenti configurazioni topologiche:
\begin{itemize}
	\item[$\circ$] \textbf{EIB\_2RING\_2TRANS}: configurazione con due ring, uno orario e uno antiorario, due transazioni contemporanee per ciascun ring (Figura~\ref{fig:EIB-2RING});
	\item[$\circ$] \textbf{EIB\_2RING\_5TRANS}: configurazione con due ring e cinque transazioni contemporanee per ciascuno di essi (Figura~\ref{fig:EIB-2RING});
	\item[$\circ$] \textbf{EIB\_4RING\_2TRANS}: configurazione con quattro ring, due orari e due antiorari, due transazioni contemporanee per ciascun ring (Figura~\ref{fig:EIB-4RING});
	\item[$\circ$] \textbf{EIB\_4RING\_5TRANS}: configurazione con quattro ring e cinque transazioni contemporanee per ciascun ring (Figura~\ref{fig:EIB-4RING});
	\item[$\circ$] \textbf{EIB\_8RING\_2TRANS}: configurazione con otto ring, quattro orari e quattro antiorari, due transazioni contemporanee per ciascun ring (Figura~\ref{fig:EIB-8RING});
	\item[$\circ$] \textbf{EIB\_8RING\_5TRANS}: configurazione con otto ring e cinque transazioni contemporanee per ciascuno (Figura~\ref{fig:EIB-8RING}).
\end{itemize}
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:EIB-2RING}]{\includegraphics[scale = 0.7]{figure/EIB-2RING}}\qquad
	\subfigure[\label{fig:EIB-4RING}]{\includegraphics[scale = 0.7]{figure/EIB-4RING}}\qquad
	\subfigure[\label{fig:EIB-8RING}]{\includegraphics[scale = 0.7]{figure/EIB-8RING}}
	\caption{Configurazioni topologiche testate per EIB: (\ref{fig:EIB-2RING}) EIB\_2RING, (\ref{fig:EIB-4RING}) EIB\_4RING e (\ref{fig:EIB-8RING}) EIB\_8RING}
	\label{fig:EIB-topologies}
\end{figure}

\section{Configurazioni topologiche adottate per $\times$pipes}
Per la network on chip $\times$pipes sono state scelte e testate le configurazioni topologiche più frequentemente incontrate in letteratura:
\begin{itemize}
	\item[$\circ$] \textbf{Crossbar}. Tale topologia permette di interconnettere, con un unico switch $10 \times 10$ ad alto volume, tutti i moduli del simulare (Figura~\ref{fig:xbar}).
	\item[$\circ$] \textbf{Mesh}. La mesh (griglia) è caratterizzata da sei switch $4 \times 4$ e da quattro switch $3 \times 3$ per un totale di dieci switch, pari al numero di elementi da interconnettere, organizzati come in Figura~\ref{fig:mesh}.
	\item[$\circ$] \textbf{Spidergon}. La spidergon è costituita da dieci switch $4 \times 4$ connessi come in Figura~\ref{fig:spidergon}. In figura è possibile notare come il numero massimo di \emph{hops} tra i vari switch sia limitato dalle connessioni incrociate che permettono di collegare direttamente gli switch che si trovano alle estremità opposte all'interno della rete.
	\item[$\circ$] \textbf{Torus}. La topologia torus è molto simile alla mesh, rispetto a questa però offre delle connessioni ulteriori tra switch estremali nella rete (Figura~\ref{fig:torus}).
\end{itemize}
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:xbar}]{\includegraphics[width = 7cm]{figure/xbar}}\qquad
	\subfigure[\label{fig:mesh}]{\includegraphics[width = 7cm]{figure/mesh}}\qquad
	\subfigure[\label{fig:spidergon}]{\includegraphics[width = 7cm]{figure/spidergon}}\qquad
	\subfigure[\label{fig:torus}]{\includegraphics[width = 7cm]{figure/torus}}	
	\caption{Configurazioni topologiche per $\times$pipes: (\ref{fig:xbar}) crossbar, (\ref{fig:mesh}) mesh, (\ref{fig:spidergon}) spidergon e (\ref{fig:torus}) torus}
	\label{fig:NoC-topologies}
\end{figure}

\section{Benchmark e metriche utilizzati per la caratterizzazione delle performance}
La caratterizzazione delle prestazioni di EIB e $\times$pipes è stata eseguita facendo uso dei seguenti benchmark:
\begin{itemize}
	\item[$\circ$] \textbf{DMA cycle-1hop}: prevede lo scambio di dati da parte di ogni SPE con le SPE sue vicine; questo permette di saturare il mezzo di interconnessione ed è perciò ideale per caratterizzarne le prestazioni;
	\item[$\circ$] \textbf{DMA cycle-5hop}: prevede lo scambio di dati da parte di ogni SPE con le SPE che si trovano a una distanza di 5 hop (ovvero la massima distanza tra due moduli all'interno della rete);
	\item[$\circ$] \textbf{DMA memory}: prevede lo scambio di dati tra tutte le SPE e la memoria. Come mostrato nella parte conclusiva del capitolo, quest ultimo evidenzia una grande differenza nei risultati tra EIB e $\times$pipes.
\end{itemize}
Le metriche adottate per la valutazione delle performance sono:
\begin{itemize}
	\item[$\circ$] \textbf{Latenza}: numero di cicli che ciascun memory access impiega ad essere trasferito dal buffer di ingresso, del modulo corrispondente al mezzo di interconnessione, al buffer di uscita dello stesso;
	\item[$\circ$] \textbf{Execution time}: numero di cicli di sistema necessari al completamento dell'esperimento. \end{itemize}
%La raccolta e l'estrazione dei dati necessari alla caratterizzazione delle infrastrutture di comunicazione, ha richiesto la revisione della classe \emph{MemoryAccess} del simulatore. Ogni informazione relativa a latenza e distribuzione del carico di lavoro tra i diversi ring (nel caso di EIB) è stata inserita all'interno di tale classe. Per quanto riguarda la network on chip'interno del wrapper $\times$pipes e del modulo di EIB è stato inserito un contatore (\emph{cycle\_number}) utilizzato per la rilevazione del numero di cicli di esecuzione dell'applicazione. Lo stesso contatore è stato anche utilizzato per la misurazione della latenza dei memory access. 

\section{Risultati EIB ottenuti tramite benchmark DMA cycle}
Vengono di seguito analizzati i dati raccolti per EIB tramite il benchmark DMA cycle. Come visto, per tale benchmark sono disponibili due configurazioni che differiscono per l'organizzazione delle transazioni tra le SPE del simulatore. In particolare, la configurazione 1 hop prevede che ogni SPE comunichi con le sue vicine (distanti appunto un 1 hop), mentre la configurazione 5 hop prevede che ogni SPE comunichi con quelle a distanza di 5 salti. Quest'ultima tiene conto del fatto che nel Cell è possibile distribuire le immagini del programma sia manualmente, da parte del programmatore, che in modo automatico, lasciando la scelta al compilatore; in tal caso potrebbe capitare che la disposizione delle immagini tra le SPE non sia tale da minimizzare il numero di hop (comunicazione tra SPE adiacenti).\\
Le transazioni DMA consistono nel trasporto di dati tra le local store delle SPE coinvolte. Ogni SPE effettua una serie di trasferimenti DMA, di dimensione pari a 64-byte (o 128-byte, 256-byte, \ldots, fino a un massimo di 16384-byte) per un totale di 32-MB, tra la propria LS e quella della SPE target.\\
Ogni esperimento è stato ripetuto variando il numero di ring dal valore minimo, pari a due (Figura~\ref{fig:EIB-2RING}), fino al valore massimo, pari a otto (Figura~\ref{fig:EIB-8RING}). Inoltre, per ogni configurazione di ring (2,4 e 8) sono state fatte variare il numero massimo di transazioni possibili per ciascuno. In definitiva ogni prova è stata eseguita combinando i due gradi di liberta a disposizione: numero di ring e numero massimo di transazioni per ogni ring.   

\subsection{Execution Time}
In Figura~\ref{fig:EIB-cycle-1hop-exec-time} è riportato il grafico relativo al tempo di esecuzione dell'applicazione DMA cycle-1hop. In ascisse è espressa la configurazione topologica, mentre in ordinate è riportato il miglioramento percentuale, in termini di riduzione del numero di cicli, rispetto alla configurazione di riferimento, EIB\_4RING\_5TRANS. 
\begin{figure}[!htbp]
	\centering
	\includegraphics[height=5cm]{figure/EIB-cycle-1hop-exec-time}
	\caption{Miglioramento percentuale dell'execution time al variare della topologia}
	\label{fig:EIB-cycle-1hop-exec-time}
\end{figure}
E' evidente un netto miglioramento nel passaggio da 2 ring (EIB\_2RING\_2TRANS) a 4 ring (EIB\_4RING\_2TRANS), oltre il 20\%, che corrisponde a una differenza di oltre 3 milioni di cicli. Aumentando ulteriormente il numero di ring a 8, il miglioramento continua a persistere, anche se in modo meno marcato, arrivando fino ad un massimo di 2.96\%, nel caso di topologia EIB\_8RING\_5TRANS.\\
L'improvement ottenuto passando da 2 ring a 4 è riconducibile al fatto che durante la simulazioni sono contemporaneamente attive otto transazioni (Figura~\ref{fig:EIB-cycle-1hop}), a ciascuna delle quali corrispondono una load e una store di risposta, mentre il numero massimo supportabile dalla configurazione EIB\_2RING\_2TRANS è pari a 4, ovvero la metà. 
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale = 0.7]{figure/EIB-cycle-1hop}
	\caption{Distribuzione delle transazioni tra le varie SPE per DMA cycle-1hop}
	\label{fig:EIB-cycle-1hop}
\end{figure}
In Figura~\ref{fig:EIB-cycle-1hop-congestion} sono riportate le cause di congestione per le diverse topologie analizzate, per memory access store di size pari a 1024-byte. 
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:EIB22-cycle-1hop}]{\includegraphics[width = 7cm]{figure/EIB22-cycle-1hop-congestion}}\qquad
	\subfigure[\label{fig:EIB25-cycle-1hop}]{\includegraphics[width = 7cm]{figure/EIB25-cycle-1hop-congestion}}\qquad
	\subfigure[\label{fig:EIB42-cycle-1hop}]{\includegraphics[width = 7cm]{figure/EIB42-cycle-1hop-congestion}}\qquad
	\subfigure[\label{fig:EIB45-cycle-1hop}]{\includegraphics[width = 7cm]{figure/EIB45-cycle-1hop-congestion}}\qquad	
	\subfigure[\label{fig:EIB82-cycle-1hop}]{\includegraphics[width = 7cm]{figure/EIB82-cycle-1hop-congestion}}
	\caption{In figura è indicato, all'interno delle fette delle torte, il numero di memory access store di size 1024-byte corrispondenti alla causa di congestione. Il numero totale di memory access è dato da $32 \times 1024 = 32768$. (\ref{fig:EIB22-cycle-1hop}) EIB\_2RING\_2TRANS, (\ref{fig:EIB25-cycle-1hop}) EIB\_2RING\_5TRANS, (\ref{fig:EIB42-cycle-1hop}) EIB\_4RING\_2TRANS, (\ref{fig:EIB45-cycle-1hop}) EIB\_4RING\_5TRANS e (\ref{fig:EIB82-cycle-1hop}) EIB\_8RING\_2TRANS}
	\label{fig:EIB-cycle-1hop-congestion}
\end{figure}
E' possibile osservare la scomparsa di \emph{Not Available Transaction} e \emph{Arbitration Lost} nel passaggio da EIB\_2RING\_2TRANS a EIB\_2RING\_5TRANS. Ciò è dovuto al fatto che EIB\_2RING\_5TRANS offre un numero di transazioni tale da permettere la comunicazione di tutte le SPE. Nonostante questo, il numero di cicli di esecuzione continua a restare elevato poiché, essendo disponibili due soli ring, gli \emph{overlapping} seguitano ad essere fortemente penalizzanti, in quanto non permettono di sfruttare appieno la banda disponibile.\\
Passando alla configurazione EIB\_4RING\_2TRANS, le transazioni hanno a disposizione due ring aggiuntivi che possono utilizzare per ridurre il numero di \emph{overlapping}; infatti, questi si riducono di oltre la metà rispetto a EIB\_2RING\_2TRANS. Se si continua a procedere verso configurazioni con numero maggiore di ring e transazioni, il miglioramento delle prestazioni si fa meno evidente, come si può notare si riesce ad ottenere una forte riduzione degli \emph{overlapping}, mentre gli \emph{overlapping target} continuano a sussistere, poiché non possono essere ridotti aumentando ring e transazioni. A questo punto è necessaria una precisazione; è possibile osservare in Figura~\ref{fig:EIB-cycle-1hop-congestion} una variazione nel numero di \emph{overlapping target} da una configurazione all'altra, mentre è stato appena detto che tale quantità è indipendente da quanti ring e transazioni siano disponibili. Quanto espresso è vero, c'è da aggiungere che agendo sui gradi di libertà a disposizione cambiano le configurazioni dei pattern di traffico sul mezzo di interconnessione e questo produce un diverso concatenamento delle richieste da parte delle SPE, con conseguente variazione delle cause di congestione, compreso \emph{overlapping target}.\\

In Figura~\ref{fig:EIB-cycle-5hop} è riportata la configurazione delle transazioni all'interno di EIB per il benchmark DMA cycle-5hop. I risultati relativi all'execution time sono invece mostrati in Figura~\ref{fig:EIB-cycle-5hop-exec-time}.
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale=0.7]{figure/EIB-cycle-5hop}
	\caption{Distribuzione delle transazioni tra le varie SPE per DMA cycle-5hop}
	\label{fig:EIB-cycle-5hop}
\end{figure}
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale = 0.7]{figure/EIB-cycle-5hop-exec-time}
	\caption{Miglioramento percentuale dell'execution time al variare della topologia}
	\label{fig:EIB-cycle-5hop-exec-time}
\end{figure}
In quest'ultima il divario tra la configurazione EIB\_2RING\_2TRANS e EIB\_4RING\_2TRANS è ancora più marcato rispetto al benchmark DMA cycle-1hop e pari a oltre 30\%. Ciò è dovuto al fatto che in DMA cycle-5hop si hanno un gran numero di \emph{overlapping} che possono essere ridotti solo aumentando il numero di ring (Figura~\ref{fig:EIB-cycle-5hop-congestion}). Nonostante ciò, l'execution time non subisce miglioramenti consistenti a fronte di un aumento del numero di ring oltre le 4 unità. Ancora una volta, tale comportamento può trovare giustificazione nel fatto che variando le configurazioni topologiche le transazioni in atto si incrociano diversamente modificando i pattern di traffico su EIB; a sostegno di tale tesi si può notare come il numero di \emph{overlapping target} aumenti passando da EIB\_4RING\_5TRANS a EIB\_8RING\_2TRANS.
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:EIB22-cycle-5hop}]{\includegraphics[width = 7cm]{figure/EIB22-cycle-5hop-congestion}}\qquad
	\subfigure[\label{fig:EIB25-cycle-5hop}]{\includegraphics[width = 7cm]{figure/EIB25-cycle-5hop-congestion}}\qquad
	\subfigure[\label{fig:EIB42-cycle-5hop}]{\includegraphics[width = 7cm]{figure/EIB42-cycle-5hop-congestion}}\qquad
	\subfigure[\label{fig:EIB45-cycle-5hop}]{\includegraphics[width = 7cm]{figure/EIB45-cycle-5hop-congestion}}\qquad	
	\subfigure[\label{fig:EIB82-cycle-5hop}]{\includegraphics[width = 7cm]{figure/EIB82-cycle-5hop-congestion}}\qquad
	\subfigure[\label{fig:EIB85-cycle-5hop}]{\includegraphics[width = 7cm]{figure/EIB85-cycle-5hop-congestion}}
	\caption{In figura è indicato, all'interno delle fette delle torte, il numero di memory access store di size 1024-byte corrispondenti alla causa di congestione. Il numero totale di memory access è dato da $32 \times 1024 = 32768$ corrispondenti alla causa di congestione. (\ref{fig:EIB22-cycle-5hop}) EIB\_2RING\_2TRANS, (\ref{fig:EIB25-cycle-5hop}) EIB\_2RING\_5TRANS, (\ref{fig:EIB42-cycle-5hop}) EIB\_4RING\_2TRANS, (\ref{fig:EIB45-cycle-5hop}) EIB\_4RING\_5TRANS, (\ref{fig:EIB82-cycle-5hop}) EIB\_8RING\_2TRANS e (\ref{fig:EIB85-cycle-5hop}) EIB\_8RING\_5TRANS}
	\label{fig:EIB-cycle-5hop-congestion}
\end{figure} 
\newpage
\subsection{Latency}
In Figura~\ref{fig:av-latency-cycle-1hop} sono riportati i dati relativi alla latenza media dei memory access store all'interno di EIB, per il benchmark DMA cycle-1hop. In ascisse vengono rappresentate le dimensioni dei trasferimenti e in ordinate la variazione percentuale della latenza media rispetto alla configurazione topologica di riferimento (EIB\_4RING\_5TRANS). A differenza della metrica precedente (execution time), ora variazioni positive indicano un aumento di latenza e quindi un degrado delle prestazioni. 
\begin{figure}[!htbp]
	\centering
	\includegraphics[height=5cm]{figure/av-latency-cycle-1hop}
	\caption{Aumento percentuale del numero di cicli di latenza per la varie topologie, rispetto alla configurazione di riferimento EIB\_4RING\_5TRANS}
	\label{fig:av-latency-cycle-1hop}
\end{figure}
Nel grafico si può evincere come aumentando il numero di ring e di transazioni la latenza tenda a diminuire. I risultati più scadenti sono, come auspicato, quelli relarivi a EIB\_2RING\_2TRANS e EIB\_2RING\_5TRANS. Le prestazioni migliori sono invece raggiunte da EIB\_8RING\_5TRANS. In quest ultimo caso, grazie all'elevato numero di ring a disposizione, i memory access riducono la loro attesa nel buffer di ingresso (Figura~\ref{fig:latency-trandy}), mentre la latenza dovuta alla serializzazione del messaggio (\emph{serialization latency} evidenziata in verde in figura) è indipendente da tale parametro.
\begin{figure}[!htbp]
	\centering
	\includegraphics[scale=0.7]{figure/latency-trendy}
	\caption{Riduzione della componente di latenza legata all'attesa nel buffer di ingresso all'aumentare del numero di ring e di transazioni}
	\label{fig:latency-trandy}
\end{figure}
\\

In Figura~\ref{fig:av-latency-cycle-5hop} sono riportati i dati relativi alla latenza media dei memory access store all'interno di EIB per il benchmark DMA cycle-5hop.
\begin{figure}[!htbp]
	\centering
	\includegraphics[height=5cm]{figure/av-latency-cycle-5hop}
	\caption{Aumento percentuale del numero di cicli di latenza per la varie topologie, rispetto alla configurazione di riferimento EIB\_4RING\_5TRANS}
	\label{fig:av-latency-cycle-5hop}
\end{figure}
In questo caso il divario tra EIB\_2RING\_2TRANS e EIB\_4RING\_2TRANS è ancora più marcato rispetto a DMA cycle-1hop, in accordo con l'osservazione fatta precedentemente sulla metrica dell'execution time.

\section{Risultati EIB ottenuti tramite benchmark DMA memory}
Il benchmark DMA memory prevede la comunicazione di tutte le SPE con la memoria. Ogni SPE effettua una lettura (GET) dalla memoria principale, alla quale segue poi una scrittura sulla stessa (PUT). Anche in questo caso le dimensioni dei transferimenti variano da un minimo di 64-byte fino a un massimo di 16-KB per un totale di 32-MB, per ogni SPE e per ogni size citata. Poiché tutte le unità del sistema comunicano con la memoria, questo esperimento permette di saturarne la banda e valutare l'impatto degli approcci considerati, EIB e $\times$pipes, su tempo di esecuzione e latenza dei memory access.
 
\subsection{Execution Time}
La Figura~\ref{fig:EIB-mem-exec-time} illustra i dati riguardanti il miglioramento percentuale delle prestazioni relativamente all'execution time, per le diverse configurazioni di EIB analizzate, prendendo come riferimento EIB\_4RING\_5TRANS. Da una prima analisi è riscontrabile una certa omogeneità tra i tempi di esecuzione dell’applicazione al variare della topologia considerata. Le topologie con 2 ring si dimostrano leggermente più performanti rispetto a quelle con 4 e 8, miglioramenti di circa 3.5\%; questo può essere giustificato andando a osservare la distribuzione dei cicli di latenza per i memory access. 
\begin{figure}[!htbp]
	\centering
	\includegraphics[height=5cm]{figure/EIB-mem-exec-time}
	\caption{Miglioramento percentuale del tempo di esecuzione dell'applicazione al variare della topologia, per il benchmark DMA mem. Ancora una volta la configurazione di riferimento è EIB\_4RING\_5TRANS}
	\label{fig:EIB-mem-exec-time}
\end{figure}

\subsection{Latency}
Per capire perché le configurazioni con 2 ring diano risultati migliori delle altre si può andare a valutare la latenza dei memory access (Figura~\ref{fig:mem-latency}).
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:av-latency-mem}]{\includegraphics[height = 5cm]{figure/av-latency-mem}}
	\subfigure[\label{fig:max-latency-mem}]{\includegraphics[height = 5cm]{figure/max-latency-mem}}
	\caption{(\ref{fig:av-latency-mem}) differenza percentuale tra le latenze medie dei memory access delle diverse configurazioni rispetto a quelli della configurazione di riferimento. (\ref{fig:max-latency-mem}) differenza percentuale tra le latenze massime dei memory access delle diverse configurazioni rispetto a quelli della configurazione di riferimento}
	\label{fig:mem-latency}
\end{figure}
In Figura~\ref{fig:av-latency-mem} si può osservare come aumentando il numero di ring e di transazioni la latenza media diminuisca, poiché aumenta la banda disponibile. Nonostante ciò, il numero di cicli di esecuzione dell'applicazione è minore per le configurazioni con 2 ring. Questo è dovuto principalmente al forte aumento della latenza massima dei memory access all'interno di EIB, visibile in Figura~\ref{fig:max-latency-mem}. Infatti, esclusi i memory access di 4-byte, EIB\_2RING\_2TRANS ed EIB\_2RING\_5TRANS riportano delle latenze massime che si attestano intorno a meno della metà di quelle relative a EIB\_4RING\_5TRANS, mentre per le altre topologie sono molto più alte. La presenza di memory access, anche se pochi, con latenze molto elevate provoca delle lunghe code di attesa che fanno lievitare il tempo di esecuzione dell'esperimento.
%\newpage  
\section{Confronto delle prestazioni tra EIB e $\times$pipes}
Vengono di seguito messe a confronto le prestazioni ottenute con le due tipologie di infrastrutture di comunicazione, EIB e $\times$pipes. L'analisi viene effettuata per le due metriche considerate tra i diversi benchmark utilizzati. 

\subsection{Benchmark DMA cycle}
La Figura~\ref{fig:NoC-cycle-exec-time} riporta i grafici relativi al tempo di esecuzione dell'applicazione, per il benchmark DMA cycle-1hop e DMA cycle-5hop, per tutte le topologie di NoC, rispetto alle configurazioni di EIB.
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:NoC-cycle-1hop-exec-time}]{\includegraphics[height = 6cm]{figure/NoC-cycle-1hop-exec-time}}
	\subfigure[\label{fig:NoC-cycle-5hop-exec-time}]{\includegraphics[height = 6cm]{figure/NoC-cycle-5hop-exec-time}}
	\caption{Miglioramento percentuale del tempo di esecuzione per le topologie NoC nel caso di (\ref{fig:NoC-cycle-1hop-exec-time}) DMA cycle-1hop e (\ref{fig:NoC-cycle-5hop-exec-time}) DMA cycle-5hop}
	\label{fig:NoC-cycle-exec-time}
\end{figure}
Si può notare come nel caso di 1hop tutte le topologie di NoC diano performance superiori al 10\% rispetto a EIB\_4RING\_5TRANS. Tale margine di miglioramento si assottiglia passando a 5hop (Figura~\ref{fig:NoC-cycle-5hop-exec-time}). Nel caso della spidergon il deterioramento è palese e porta le prestazioni a diventare paragonabili con quelle di EIB\_8RING\_5TRANS. Le altre topologie, esclusa la crossbar, soffrono meno l'aumento del numero di salti tra le SPE e continuano a mostrare un miglioramento di circa il 5\%.\\

In Figura~\ref{fig:NoC-cycle-latency} sono rappresentati i grafici relativi alla latenza media dei memory access all'interno di $\times$pipes, per le configurazioni 1hop e 5hop.
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:NoC-cycle-1hop-latency}]{\includegraphics[height = 5cm]{figure/NoC-cycle-1hop-latency}}
	\subfigure[\label{fig:NoC-cycle-5hop-latency}]{\includegraphics[height = 5cm]{figure/NoC-cycle-5hop-latency}}
	\caption{Andamento percentuale della latenza media al variare della dimensione del messaggio per le topologie NoC nel caso di (\ref{fig:NoC-cycle-1hop-latency}) DMA cycle-1hop e (\ref{fig:NoC-cycle-5hop-latency}) DMA cycle-5hop}
	\label{fig:NoC-cycle-latency}
\end{figure}
La topologia crossbar (xbar) è evidentemente la più perfomante, in entrambe le tipologie di benchmark (1hop e 5hop) grazie alla possibilità di collegare direttamente qualunque coppia di elementi all'interno del sistema. Tale caratteristica la rende particolarmente adatta alle configurazioni di traffico casuale (in cui tutti comunicano con tutti in modo equiprobabile). La configurazione EIB\_8RING\_5TRANS tiene testa ai valori di latenza delle topologie NoC e in certi casi si dimostra perfino migliore, soppratutto quando il numero di salti viene aumentato a 5. In questo caso gli switch condivisi dalle diverse transazioni fanno lievitare il tempo di attesa dei messaggi, anche se complessivamente il tempo di esecuzione, come visto non ne risente. %Il picco di peggioramento per le size da quattro è dovuto al fatto che queste sono utilizzate principalmente dalla PPE per inviare comandi DMA alle mailbox delle SPE, questi si sovrappongono a transazioni di altro tipo e perciò posssono incontrare congestione sul mezzo di comunicazione.
In ultima analisi, in Figura~\ref{fig:DMA-cycle-difference} è riportato il peggioramento percentuale, in termini di cicli di esecuzione dell'esperimento, nel passaggio da 1hop a 5hop nel benchmark DMA cycle. Ogni percentuale è riferita al corrispondente tempo di esecuzione nel caso migliore (1hop).
\begin{figure}[!htbp]
	\centering
	\includegraphics[height = 6cm]{figure/DMA-cycle-difference}
	\caption{Peggioramento percentuale del tempo di esecuzione per ogni topologia nel passaggio da 1 a 5 hop}
	\label{fig:DMA-cycle-difference}
\end{figure}
Come auspicabile la crossbar non subisce variazioni, mentre tutte le altre topologie subiscono un peggioramento più o meno forte. Il deterioramento maggiore, di ben il 15\%, è riscontrato per la spidergon, mesh e torus invece si aggirano intorno all'8\%. Le configurazioni di EIB a 4 e 8 ring sono più stabili delle topologie NoC e danno una perdita di circa il 3-4\%. Questo è dovuto alla possibilita di parallelizzare le comunicazioni, che subiscono \emph{overlapping}, su diversi ring.
 
\subsection{Benchmark DMA memory}
In Figura~\ref{fig:NoC-mem-exec-time} è riportato il confronto tra le topologie di NoC testate e le configurazioni di EIB precedentemente esaminate; 
\begin{figure}[!htbp]
	\centering
	\includegraphics[height = 6cm]{figure/NoC-mem-exec-time}
	\caption{Confronto tra le topologie di NoC e le configurazioni di EIB in termini di riduzione percentuale del numero di cicli di esecuzione dell'esperimento}
	\label{fig:NoC-mem-exec-time}
\end{figure}
ancora una volta il miglioraremento delle prestazioni è espresso come percentuale di numero di cicli risparmiati nell'esecuzione dell'applicazione. Nel caso in esame è evidente come tutte le topologie di NoC diano un risparmio superiore al 40\% rispetto a EIB\_4RING\_5TRANS.\\
Un'analisi più approfondita può essere fatta analizzando le latenze dei memory access (Figura~\ref{fig:NoC-mem-latency}). 
\begin{figure}[!htbp]
	\centering
	\includegraphics[height = 5cm]{figure/NoC-mem-latency}
	\caption{Confronto della latenza media tra le topologie di NoC e le configurazioni di EIB}
	\label{fig:NoC-mem-latency}
\end{figure}
Si noti come, escludendo le size da 4-byte, la latenza dei memory access, per tutte le topologie di $\times$pipes, abbia un andamento quasi sovrapposto e molto inferiore a quello osservabile con EIB\_4RING\_5TRANS. Questo è vero per dimensioni dei messaggi inferiori a 2048-byte, mentre oltre tale valore la latenza torna ad essere paragonabile e perfino superiore a quella di EIB\_4RING\_5TRANS. La giustificazione nella grande differenza sul tempo di esecuzione può essere trovata nel tipo di arbitraggio distribuito offerto da $\times$pipes e nella disponibilità di risorse di buffering distribuite tra gli switch della rete. Nel caso di EIB gli \emph{overlapping target} non permettono di sfruttare appieno la banda del mezzo di comunicazione. Essendo tutte le transazioni con la memoria, il collo di bottiglia in questo caso è dato proprio dal memory wall. L'arbitraggio distribuito permette di sfruttare in modo più efficiente le risorse di comunicazione e fornisce alla NoC un vantaggio considerevole nei confronti di EIB.\\
La Crossbar risulta leggermente meno performante rispetto alle altre topologie di NoC; questo può essere attribuito al fatto che l'arbitraggio è centralizzato e le risorse di bufferizzazione lungo il percorso sono inferiori.

\section{Calcolo della latenza in caso di \emph{overlapping target} nelle due infrastrutture analizzate}
Quando abbiamo analizzato i risultati relativi al benchmark DMA memory, abbiamo detto che il vantaggio della NoC era attribuibile alla disponibilità di arbitraggio e risorse di bufferizzazione ditribuiti tra i vari switch della rete. Vediamo di andare a calcolare la latenza dei messaggi nel seguente scenario. Supponiamo che la SPE2 e la SPE3 vogliano mandare un memory access store, di dimensione 128-byte, alla memoria nello stesso momento. Valutiamo cosa accade nel caso di EIB e di $\times$pipes.

\subsection{EIB}
I dati a nostra disposizione per il calcolo sono:
\begin{itemize}
	\item[$\circ$] banda disponibile tra SPE e memoria: 8-byte/ciclo;
	\item[$\circ$] arbitraggio centralizzato: se due SPE cercano contemporaneamente di accedere alla memoria, la priorità viene data a quella con identificativo più basso all'interno del bus;
	\item[$\circ$] configurazione topologica: qualunque tra quelle viste.
\end{itemize}
Per le due SPE la latenza dovuta alla serializzazione della richiesta verso il mezzo di interconnessione è data da:\\
$$ serialization \ latency = \frac{128(byte)}{8(byte/ciclo)} = 16(cicli)$$\\
A questa va aggiunta la latenza legata al trasferimento del comando, che supporremo pari a 2 cicli di clock (trasporto delle informazioni aggiuntive come \_size, \_source, \_target, ecc\ldots) e quella dovura agli address concentrator (AC) pari a 4 cicli per entrambe le SPE. Inoltre per la SPE2 vanno sommati anche il numero di salti necessari a raggiungere la memoria, pari a 5, che diventano 6 per la SPE3. Complessivamente avremmo una latenza per i due memory access, separatamente, pari a:\\
$$ SPE2 \ latency = 16 + 2 + 4 + 5= 27 cicli $$ \\
$$ SPE3 \ latency = 16 + 2 + 4 + 6 = 28 cicli $$\\
A causa dell'\emph{overlapping\_target} la latenza effettiva per la SPE3 è data dalla somma delle due:\\
$$ SPE3 \ latency = 27 + 28 = 55 cicli $$\\

\subsection{NoC}
I dati corrispondenti alla NoC sono i seguenti:
\begin{itemize}
	\item[$\circ$] configurazione topologica: mesh;
	\item[$\circ$] risorse di buffering: 3 out\_buffer in ogni switch della rete più altri 3 nella NI;
	\item[$\circ$] banda disponibile tra SPE e memoria: 8-byte/ciclo;
	\item[$\circ$] arbitraggio: distribuito.
\end{itemize}
Nella NoC, le due transazioni viaggiano parallelamente; la prima che occupa la memoria è quella proveniente dalla SPE2, poiché posta a un numero di hop inferiore rispetto alla SPE3. Lo scenario descritto è configurato in Figura~\ref{fig:mesh-performance}; in rosso sono evidenziati gli IP core interessati, in verde gli switch e le NI utilizzati da SPE2 e infine in arancione sono evidenziati quelli utilizzati dalla SPE3.
\begin{figure}[!htbp]
	\centering
	\subfigure[\label{fig:mesh-performance1}]{\includegraphics[width = 7cm]{figure/mesh-performance1}}\qquad
	\subfigure[\label{fig:mesh-performance2}]{\includegraphics[width = 7cm]{figure/mesh-performance2}}
	\caption{Configurazione delle transazioni tra le SPE2 e SPE3 con la memoria. (\ref{fig:mesh-performance1}) La SPE2 comunica con la memoria mentre il flit di head corrispondente alla transazione proveniente dalla SPE3 attende nel buffer dello switch numero nove (SW9); (\ref{fig:mesh-performance2}) la SPE2 termina la transazione e i flit bufferizzati nel tragitto tra SPE3 e memoria possono attraversare lo SW0 e raggiungere la destinazione}
	\label{fig:mesh-performance}
\end{figure}
Ogni switch e ogni NI nella rete introducono due cicli di latenza per ogni flit. Il numero di flit totali che devono essere inviati è pari a:\\
$$ num \ flit = \frac{128 \ byte}{8 \ byte/flit} (payload) + 1 \ flit (head) = 17 \ flit $$\\
Per la SPE2 a questi vanno aggiunti un numero di cicli:\\
$$ total \ latency = num \ flit + [5 (switch) + 2 (ni)] \times 2 = 31 $$\\
Nel frattempo però anche la SPE3 inizia la trasmissione dei suoi 128-byte che, dopo aver attraversato 5 dei 6 switch che portano alla memoria, iniziano ad essere bufferizzati all'interno della rete, poiché il sesto è occupato dalla trasmissione della SPE2. La capacità di memorizzazione della NoC nel caso della SPE3 è data da:\\
$$ cap \ NoC = [5(switch) + 1(ni)]\times 3 = 18flit $$\\
quindi superiore di una unità al numero di cui è composto l'intero messaggio. Quando la SPE2 termina la trasmissione, quella della SPE3 può continuare. In questo caso il primo flit, quello di head, non parte da una distanza di 6 salti (come sarebbe successo se la SPE3 avesse trasmesso da sola) ma parte da un solo salto. La latenza complessiva del messaggio inviato da SPE3 è data da:\\
$$ SPE3 \ latency = SPE2 \ latency + 1(sw9) + 2(sw0) + 2(ni0) + 17(flit) = 31 + 22 = 53 cicli $$\\
Si risparmiano quindi 9 cicli, rispetto al caso in cui le due SPE trasmettono una dopo l'altra.\\

In questo esempio, il numero di cicli di latenza per la SPE3, nel caso della NoC, è di poco inferiore rispetto a EIB; nonostante ciò, ora la memoria non ha tempi morti di attesa tra le due transazioni. Mentre per EIB, tra la trasmissione di un memory access e l'altro, intercorre un tempo pari al numero di salti che separano memoria e SPE3 (pari a 6), nel caso della NoC quando l'ultimo flit di tail attraversa lo SW0, lo SW9 può iniziare ad inviare i flit verso quest ultimo riducendo a 0 il tempo morto, con un risparmio di 6 cicli. Questi, moltiplicati per tutte le transazioni che avvengono contemporaneamente, danno un grosso vantaggio alla NoC rispetto a EIB sia in termini di tempo di esecuzione che in termini di latenza media dei memory access. 

\section{Conclusioni}

L'element interconnect bus del Cell è uno strumento estremamente efficiente e offre elevate prestazioni nel caso di comunicazione tra le SPE interne al sistema. L'aumento del numero di ring oltre le 4 unità non porta benefici considerevoli a fronte di un aumento di area e di consumo di potenza, ciò giustifica la scelta di 4 ring adottata nella versione commerciale del CBE. Quando le transazioni con la memoria si fanno intensive, EIB inizia a subire dei deterioramenti nelle prestazioni legati allo sfruttamento meno efficente della banda. L'utilizzo di network on chip packet switched, come $\times$pipes, permette di sfruttare in modo migliore la banda della memoria e porta perciò vantaggi significativi su tempo di esecuzione e latenza media dei messaggi. Le topologie di NoC offrono buone prestazioni anche nel caso di comunicazione interna al sistema (DMA cycle) e perciò rappresentano una valida alternativa a EIB. Da notare infine che, nel caso di applicazione DMA cycle-1hop, le prestazioni tra mesh, spidergon e torus sono molto simili in quanto le transazioni avvengono tra SPE adiacenti e le configurazioni di traffico sulla rete sono praticamente le stesse. La disposizione delle unità funzionali all'interno del sistema non permette di organizzare in modo differente gli switch limitando così i gradi di libertà. Futuri sviluppi del lavoro potrebbero essere orientati all'aumento del numero di SPE e all'utilizzo di topologie più articolate. 