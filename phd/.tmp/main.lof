\select@language {english}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces HPC I/O Stack\relax }}{8}{figure.caption.2}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces MERCURY I/O software stack. \textit {Assisted I/O library} and \textit {Advice Manager} communicate through UNIX domain sockets. The AM binds its socket to the local file system pathname \texttt {/tmp/channel}, while the AIO connects its socket to the same pathname; exactly in the same way they would bind and connect to an IP address if they were located on different nodes in the network. Unix domain sockets are used to pass ancillary data as well as custom messages between the two software entities. Data can reside in a local Linux file system, in Lustre or in GPFS.\relax }}{18}{figure.caption.5}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Detailed architecture for the \textit {Advice Manager} (AM) component. This can be further divided into three blocks: \textit {Request Manager} (RM), \textit {Register Log} (RL), and \textit {Advisor Thread} (AT).\relax }}{19}{figure.caption.6}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Simplified function call graph for the read operation in Lustre. For page operations in the Linux kernel the picture also shows the call graph typically followed by local reads as well as the call graph for the \texttt {POSIX\_FADV\_WILLNEED} advice in the \texttt {posix\_fadvise()} implementation (dashed line).\relax }}{22}{figure.caption.7}
\contentsline {figure}{\numberline {3.4}{\ignorespaces I/O read profile of the target application under analysis (\ref {figure: iopat_profile}), extracted from the the GPFS file system in the test cluster, and zoomed window (\ref {figure: iopat_zoom}) showing the actual pattern details.\relax }}{25}{figure.caption.8}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison between different usage stategies of posix\_fadvise for an input file of 55GB residing in an ext4 file system. The first bar represents the case in which no advice is used, the second bar represents the case in which a POSIX\_FADV\_WILLNEED is issued for the whole file at the beginning of the application and the third bar represents the case in which POSIX\_FADV\_WILLNEED is issued using MERCURY.\relax }}{26}{figure.caption.9}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Running time of the ROOT application for the three file system under study using different input file sizes (\ref {figure: ext4_1},~\ref {figure: gpfs_1} and~\ref {figure: lustre_1}) and different number of instances accessing a file of 5GB (\ref {figure: ext4_2},~\ref {figure: gpfs_2} and~\ref {figure: lustre_2}).\relax }}{27}{figure.caption.10}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Reads processed by local ext4, GPFS and Lustre I/O servers for various input file sizes (\ref {figure: ext4_3},~\ref {figure: gpfs_3} and~\ref {figure: lustre_3}) and multiple instances of ROOT accessing a file of 5GB (\ref {figure: ext4_4},~\ref {figure: gpfs_4} and~\ref {figure: lustre_4}).\relax }}{28}{figure.caption.11}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Example of how a 3-Dimensional dataset is represented logically in the file as a sequence of bytes and how it is finally translated in the parallel file system.\relax }}{32}{figure.caption.12}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Collective I/O write example. A data set is partitioned and assigned to six processes. Four of them work as aggregators writing data to the global file system.\relax }}{33}{figure.caption.13}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Collective I/O flow diagram for the write path in aggregators (non-aggregators neither receive nor write any data, just send it to aggregators). \texttt {MPI\_File\_write\_all()} invokes \texttt {ADIOI\_GEN\_WriteStridedColl()}. \texttt {ADIO\_WriteContig} is a macro that is replaced by \texttt {ADIOI\_GEN\_WriteContig()}. Performance critical functions for the collective I/O branch are highlighted in grey.\relax }}{34}{figure.caption.14}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Standard and modified workflows. When cache is disabled compute phase `k+1' starts after file `k' has been closed. When the cache is enabled compute `k+1' can start immediately after data has been written. At the same time, background synchronisation of cached data starts. File `k' is closed before the file `k+1' is opened, forcing the implementation to wait for cache synchronisation to complete.\relax }}{47}{figure.caption.24}
\contentsline {figure}{\numberline {4.6}{\ignorespaces coll\_perf perceived bandwidth for all combinations of <aggregators>\_<coll\_bufsize>.\relax }}{50}{figure.caption.26}
\contentsline {figure}{\numberline {4.7}{\ignorespaces coll\_perf collective I/O contribution breakdown when cache is enabled.\relax }}{51}{figure.caption.27}
\contentsline {figure}{\numberline {4.8}{\ignorespaces coll\_perf collective I/O contribution breakdown when cache is disabled.\relax }}{52}{figure.caption.28}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Flash-IO perceived bandwidth for all combinations of <aggregators>\_<coll\_bufsize>.\relax }}{53}{figure.caption.29}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Flash-IO collective I/O contribution breakdown when cache is enabled.\relax }}{54}{figure.caption.30}
\contentsline {figure}{\numberline {4.11}{\ignorespaces IOR perceived bandwidth for all combinations of <aggregators>\_<coll\_bufsize>.\relax }}{55}{figure.caption.31}
\contentsline {figure}{\numberline {4.12}{\ignorespaces IOR collective I/O contribution breakdown when cache is enabled.\relax }}{56}{figure.caption.32}
