%!TEX root = ../main.tex
\section{Introduction}
\label{sec: introduction}

The gap between hard disk drives' (HDDs) performance and processors' computing power, better known as the I/O performance gap problem, represents a serious scalability limitation especially for scientific applications running on High End Computing (HEC) clusters. 
Parallel File Systems (PFSs) such as Lustre~\cite{Braam02}, PVFS~\cite{CarnsLRT} and GPFS~\cite{SchmuckH02}, just to mention a few, try to bridge this gap by striping files across multiple storage devices and providing multiple parallel data paths to increase the aggregate I/O bandwidth and the number of IOPS. The ROMIO middleware\footnote{Implementation of MPI I/O specifications from Argonne National Laboratory included in MPICH package (http://www.mpich.org/).} implements extensions to the POSIX I/O interface typically provided by PFSs that result in a richer parallel I/O interface, and through the Abstract Device I/O (ADIO) driver~\cite{ThakurGL96} enables transparent file access optimizations based on two-phase I/O and data sieving to adapt I/O patterns to the characteristics of the underlying file system~\cite{ThakurGL99}~\cite{Ying08}~\cite{ProstTHKW00}.
%
Nevertheless, as Carns et al. have pointed out in their study~\cite{CarnsHABLLR11} most of the scientific applications running on big clusters today still use the POSIX I/O interface to access their data. %Furthermore, it has also been ascertained that using POSIX I/O to access non-contiguous regions of files causes extremely poor performance in the case of PFSs~\cite{ChingCLP06}. Indeed, PFSs provide best I/O bandwidth performance for large contiguous requests while they typically provide only a fraction of the maximum bandwidth in the opposite case. This is primarily due to the high number of remote procedure calls generated by the file system clients that overwhelms I/O servers and the resulting high number of HDDs' head movements in every I/O target (seek overhead). 

Currently there is no available solution to overcome limitations caused by non-optimal file I/O patterns generated by applications, except to re-write them. In this context, the Linux kernel provides users with the capability to communicate access pattern information to the local file system through the \texttt{posix\_fadvise()}~\cite{AdviseAPI} system call. The file system can use this information to improve page cache efficiency, for example, by prefetching (or releasing) data that will (or will not) be required soon in the future or by disabling read-ahead in the case of random read patterns. However, \texttt{posix\_fadvise()} is barely used in practice and has intrinsic limitations that discourage its employment in real applications. 
%TODO: is there a citation for this 'barely' claim?

The two most used PFSs in HEC clusters nowadays, IBM GPFS and Lustre, are both POSIX compliant. Nevertheless, neither of them support the POSIX advice mechanism previously described. GPFS compensates for the lack of POSIX advice support through a hints API that users can access by linking their programs against a service library. Hints are passed to GPFS through the \texttt{gpfs\_fcntl()}~\cite{GPFSHINTS} function and can be used to guide prefetching (or releasing) of file blocks in the page pool\footnote{GPFS pinned memory used for file system caching.}. However, unlike POSIX advice, GPFS hints %are not mandatory and 
can be discarded by the file system if certain requirements are not met. Lustre, on the other hand, does not provide any client side mechanism similar to GPFS hints or POSIX advice. Recently a new Lustre advice mechanism has been proposed by DDN during the Lustre User Group 2014 (LUG14) in Miami~\cite{Comer14}. The difference in the DDN approach is that it provides control over the storage servers (OSSs) cache instead of the file system client cache.
%TODO: why is your approach better? Yours has the penalty of moving the data over the wire into the client cache. Perhaps the approaches are complimentary, you address "I will want this data", they address "someone will want this data, but _they_ may not know it yet"

In this paper we propose and evaluate a novel guided I/O framework called "MERCURY"~\cite{mercury} able to optimize file access patterns at run-time through data prefetching using available hints mechanisms. MERCURY communicates file I/O pattern information to the file system on behalf of running applications using a dedicated process that we call \textit{Advice Manager}. In every node of the cluster, processes can access their files using an \textit{Assisted I/O library} that transparently forwards intercepted requests to the local \textit{Advice Manager}. This uses \texttt{posix\_fadvise()} and \texttt{gpfs\_fcntl()} to prefetch (or release) data into (or from) the client's file system data cache. The \textit{Assisted I/O library} controls for which files advice or hints should be given, while the \textit{Advice Manager} controls how much data to prefetch (or release) from each file. Monitored file paths and prefetching information are contained in a configuration file that can be generated either manually or automatically once the I/O behaviour of the target application is known. The configuration file mechanism allows us to decouple the specific hints API provided by the back-end file system from the generic interface exposed to the final user thus making our solution portable.

With this approach we are able to generate POSIX advice and GPFS hints for applications that do not use them but can receive a benefit from their use. We accomplish this asynchronously, %with very low overhead, 
and without any modification of the original application. We demonstrate that our approach is effective in improving the I/O bandwidth, reducing the number of I/O requests and reducing the execution time of a `ROOT' \footnote{Data analysis framework developed at CERN.}~\cite{root} based analytic application.

Additionally, we propose and evaluate a modification to the Linux kernel that makes it possible for Lustre, and in principle other networked file systems, to participate in activity triggered by the \texttt{posix\_fadvise()} system call, thus allowing it to take advantage of our guided I/O framework benefits.

The remainder of this paper is organised as follows. Section~\ref{sec: background} covers the background on file systems support to guided I/O interfaces (POSIX advice and GPFS hints), Section~\ref{sec: concept} presents concept, design and implementation of the MERCURY prototype highlighting the main contributions of the work. This section also describes the kernel modifications that enable POSIX advice on Lustre, Section~\ref{sec: evaluation} presents the evaluation of our prototype on three file systems: a local Linux ext4 file system, a GPFS file system and a Lustre file system, Section~\ref{sec: related_work} presents related works on data prefetching, and finally Section~\ref{sec: conclusion} presents conclusion and future work.    

%\todo[inline]{need to make clear that the configuration file enables user to specify prefetching information in the most generic way. Afterwards, this information is translated to match the corresponding backend file system hint interface}

%\todo[inline]{also need to add lustre to the mix of file systems. In this case there will be a dedicated section to the VFS patching to make POSIX\_FADV\_WILLNEED work with lustre}
