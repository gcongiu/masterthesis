%!TEX source = ../../main.tex
\section{The MPI-IO Hints API}
\label{mpi-io-hints}
The MPI-IO standard allows users, as well as other libraries (e.g. HDF5, pnetCDF, etc), to control the internal behavior of the MPI-IO implementation (e.g. ROMIO) through a dedicated hints API. Hints are packed into a special object of type \codeword{MPI\_Info} and passed to the \codeword{MPI\_File\_open()} function. The open function transmits the hints to the underlying software modules that interpret them and take appropriate actions. An example of collective write and read hint initialisation and submission for the ROMIO middleware is shown in Listing~\ref{list: mpi-io-hint-example}.

\begin{lstlisting}[language=C, caption=MPI-IO Hints Initialisation and Submission, label={list: mpi-io-hint-example}]
  /* info object declaration */
  MPI_Info info;

  /* info object creation */
  MPI_Info_create(&info);

  /* info object initialisation */
  MPI_Info_set(info, "romio_cb_write", "enable");
  MPI_Info_set(info, "romio_cb_read", "enable");

  /* file object declaration */
  MPI_File file;

  /* file object initialisation */
  MPI_File_open(MPI_COMM_WORLD, "test_file", MPI_MODE_RDWR, 
    info, &file);

  /* perform I/O */

  /* file object finalisation */
  MPI_File_close(&file);

  /* info object destruction */
  MPI_Info_destroy(&info);
\end{lstlisting}

ROMIO exploits the MPI-IO hints API and defines its own set of hints to control the internal I/O transport behaviour. ROMIO hints are summarised in Table~\ref{table: romio-hints}. 

\begin{table}[!htb]
\centering
\ra{1.5}
\caption{ROMIO Hints and Corresponding Description}
\newcolumntype{K}{>{\centering\arraybackslash} m{4cm}}
\newcolumntype{V}{>{\centering\arraybackslash} m{5cm}}
\begin{tabular}{KV}
\toprule
\bf \small Hint & \bf \small Description \\
\midrule
\small \ttfamily  ind\_rd\_buffer\_size & \small independent read buffer size \\
\small \ttfamily  ind\_wr\_buffer\_size & \small independent write buffer size \\
\small \ttfamily  romio\_ds\_read & \small enable data sieving for reads \\
\small \ttfamily  romio\_ds\_write & \small enable data sieving for writes \\
\small \ttfamily  cb\_buffer\_size & \small collective buffer size \\
\small \ttfamily  cb\_nodes & \small number of aggregators in collective I/O \\
\small \ttfamily  romio\_cb\_read & \small enable collective I/O for reads \\
\small \ttfamily  romio\_cb\_write & \small enable collective I/O for writes \\
\small \ttfamily  romio\_no\_indep\_rw & \small enable deferred open (only aggregators open the file) \\
\small \ttfamily  cb\_config\_list & \small list of nodes to be selected as aggregators \\
\small \ttfamily  striping\_factor & \small number of I/O targets used to store the file \\
\small \ttfamily  striping\_unit & \small size of the stripe unit used to store the file \\
\small \ttfamily  start\_iodevice & \small I/O target storing the first stripe \\
\bottomrule
\end{tabular}
\label{table: romio-hints}
\end{table}

Hints are used to control the size of the I/O buffer both in independent and collective operations (i.e. \texttt{ind\_rd\_buffer\_size}, \texttt{ind\_wr\_buffer\_size} and \texttt{cb\_buffer\_size}), to enable I/O aggregation during reads and writes (i.e. \texttt{romio\_cb\_read} and \texttt{romio\_cb\_write}), and even to control file system specific parameters such as the number of I/O targets used to store the file (\texttt{striping\_factor}) or the size of the data blocks stored by every target (\texttt{striping\_unit}). Besides the ones reported in Table~\ref{table: romio-hints} there are additional file system specific hints (e.g. Lustre, PVFS, etc) that target the corresponding ROMIO file system drivers. For simplicity these are not reported in the table above. 

Hints are not MPI-IO specific, every middleware can exploit the MPI-IO hints API to support its own hints. In Chapter~\ref{chapter: deeper} we will discuss how to extend the ROMIO hints to support local storage file caching. Since we focus on improving the collective I/O implementation in ROMIO by introducing an additional memory tier, the next section is dedicated to collective I/O hints and implementation.

\subsection{Collective I/O in ROMIO}
\label{subsec: collio}
As already mentioned in the introduction, ROMIO is a popular implementation of the MPI-IO specification developed at the Argonne National Laboratory and currently supported by MPICH as well as OpenMPI and other packages. ROMIO provides parallel I/O functionalities for different file systems through the Abstract Device I/O interface (ADIO). Latest versions of ROMIO include support for Lustre, GPFS, PVFS and others through a dedicated ADIO driver. In ROMIO collective I/O is a parallel I/O technique designed to deliver high performance data access to distributed scientific applications that need to write data to a shared file efficiently.

\subsubsection{Two Phase I/O}
\label{subsubsec: ext2ph}
The core component of collective I/O is the `two phase I/O', also known as `extended two phase algorithm' (ext2ph)~\cite{ThakurC96}. The ROMIO implementation for collective I/O consists of several steps as following described:

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{chapters/chapter3/figures/ext2ph}
  \caption{Collective I/O flow diagram for the write path in aggregators (non-aggregators neither receive nor write any data, just send it to aggregators). \codeword{MPI\_File\_write\_all()} invokes \codeword{ADIOI\_GEN\_WriteStridedColl()}. \codeword{ADIO\_WriteContig} is a macro that is replaced by \codeword{ADIOI\_GEN\_WriteContig()}. Performance critical functions for the collective I/O branch are highlighted in grey.}
  \label{figure: coll_io_impl}
\end{figure}

\begin{enumerate}
\item All processes taking part in the I/O operation exchange access pattern information with each other. The access pattern information is represented by start and end offsets for the accessed region (disregarding holes that may be present). Once file offsets are available, every process works out how big the global accessed region in the file is by taking maximum and minimum among all. The resulting byte range is divided by the number of available aggregators to build the so called `file domains' (contiguous byte ranges accessed independently by every aggregator).
\item Every process works out which file domains (and thus aggregators) its local data belongs to. In doing so, every process knows which aggregators it has to send (receive) data to (from), if any.
\item Every aggregator works out which other processes' requests map to its file domain. Doing so every aggregator knows what processes need to receive (in case of reads) or send (in case of writes) data for that particular file domain.
\item Actual two phase I/O starts. In the case of writes, that we exclusively consider here (the read case is similar), every process sends its data to the right aggregators (data shuffle phase) while these write the data to the parallel file system (data I/O phase). Data is written in blocks of predefined size (collective buffer size). If the size of the collective buffer is smaller than the file domain, the file domain is broken down into multiple sub-domains which are written in different rounds of the ext2ph algorithm. In order to handle multiple rounds of data shuffle and I/O, additional access information is required. This is disseminated by every process (collectively) to aggregators at the beginning of the data shuffle phase.
\item Once all the data has been written, all the processes must synchronise and exchange error codes. This is necessary to guarantee that it is safe to free the memory buffers containing the data.
\end{enumerate}

Figure~\ref{figure: coll_io_impl} shows how the previous steps map to the collective I/O implementation for the write operation. The collective write function (\codeword{MPI\_File\_write\_all()}) in ADIO is implemented through \codeword{ADIOI\_GEN\_WriteStridedColl()}. This is responsible for selecting the most suitable I/O method between those available. For example, independent I/O is selected if the access requests are not interleaved. Nevertheless, users can always enforce collective I/O by setting the appropriate MPI-IO hint. The \codeword{ADIOI\_Exch\_and\_write()} function contains the ext2ph algorithm implementation, including data shuffle and write methods. At the beginning of the data shuffle (\codeword{ADIOI\_W\_Exchange\_data()}) we have the dissemination function (\codeword{MPI\_Alltoall()}) used to exchange information concerning which part of the data has to be sent during a particular round of two phase I/O. 

There are three main contributors to collective I/O performance: (\textbf{a}) global synchronisation cost; (\textbf{b}) communication cost; and (\textbf{c}) write cost. \codeword{MPI\_Allreduce()} and \codeword{MPI\_Alltoall()} account for the global synchronisation cost. When a process reaches them it has to wait for all the other processes to arrive before continuing. \codeword{MPI\_Waitall()} accounts for communication cost since every process first issues all the non-blocking receives (if any) and sends, and afterwards waits for them to complete (refer to the right part of the diagram in Figure~\ref{figure: coll_io_impl}). Finally, \codeword{ADIO\_WriteContig()} accounts for write cost.

\subsubsection{Collective I/O Hints}
\label{subsec: hints}
As already said, collective I/O behaviour can be controlled by users through a dedicated set of MPI-IO hints. Users can control whether collective I/O should be enabled or disabled with \codeword{romio\_cb\_write} and \codeword{romio\_cb\_read}, for write and read operations respectively, how many aggregators should be used during a collective I/O operation with \codeword{cb\_nodes} and how big the collective buffer should be with \codeword{cb\_buffer\_size}. Table~\ref{table: coll_io_hints_table} summarises the hints just described (these are also reported in Table~\ref{table: romio-hints} along with all the other hints).

\begin{table}[!htb]
\centering
\ra{1.5}
\caption{Collective I/O hints in ROMIO.}
\newcolumntype{K}{>{\centering\arraybackslash} m{3cm}}
\newcolumntype{V}{>{\centering\arraybackslash} m{5.5cm}}
\begin{tabular}{KV}
\toprule
\bf \small Hint & \bf \small Description \\
\midrule
\small \codeword{romio\_cb\_write} & \small \codeword{enable} or \codeword{disable} collective writes \\
\small \codeword{romio\_cb\_read} & \small \codeword{enable} or \codeword{disable} collective reads \\
\small \codeword{cb\_buffer\_size} & \small set the collective buffer size [bytes]\\
\small \codeword{cb\_nodes} & \small set the number of aggregator processes\\
\bottomrule
\end{tabular}
\label{table: coll_io_hints_table}
\end{table}

Each of these hints has an effect on collective I/O performance. For example, by increasing the number of aggregators there will be a higher number of nodes writing to the parallel file system and thus a higher chance that one of these will experience variable performance due to load imbalance among available I/O servers, with increasing write time variation and associated global synchronisation cost. Furthermore, by increasing the collective buffer size users can reduce the number of two phase I/O rounds and, consequently, the number of global synchronisation events. Bigger collective buffers will also affect the write cost since more I/O servers will be accessed in parallel potentially increasing the aggregated I/O bandwidth.

Besides the hints described in Table~\ref{table: coll_io_hints_table}, there are other hints that do not directly concern collective I/O but affects its performance. The first is the \codeword{striping\_factor} hint, which defines how many I/O targets will be used to store the file. The second is the \codeword{striping\_unit} hint, which defines how big the data chunks written to each I/O target will be (in bytes). These two hints change the file characteristics in the parallel file system and typically the striping unit also defines the locking granularity for the file in the file system (e.g. Lustre).
